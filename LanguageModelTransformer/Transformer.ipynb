{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0f3df22-b132-458a-b16f-3ee0fb366c5a",
   "metadata": {},
   "source": [
    "<div align=\"center\">\r\n",
    "<h1><b>How To Build a Large Language Model</b></h1>\r\n",
    "<h3>Under 5 Minutes</h3>\r\n",
    "</div>\r\n",
    "\r\n",
    "<div align=\"left\">\r\n",
    "<p> Hey Everyone, today we are going to focus on building our own Large Language Model. Weâ€™ll cover the core backbone of how to organize, understand, and apply building such a model. This guide is broken into 8 steps: 6 for model creation, followed by exporting the model in ONNX format and deploying it into a backend service of your choice. In this example, we will use Microsoft Azure. However, you can also use Firebase, MongoDB, or even create your own backend with MySQL. The options are endless!</p>\r\n",
    "\n",
    "<div align=\"left\">\n",
    "<p><b>Overview:</b></p>\n",
    "<ol>\n",
    "    <li>Data Feature Engineering</li>\n",
    "    <li>Model Architecture</li>\n",
    "    <li>Training</li>\n",
    "    <li>Validation</li>\n",
    "    <li>Testing</li>\n",
    "    <li>Backtesting</li>\n",
    "    <li>Export the Model from PyTorch to ONNX</li>\n",
    "    <li>Deploy the Model in a Backend</li>\n",
    "</ol>\n",
    "</div>\r\n",
    "\r\n",
    "<div align=\"left\">\r\n",
    "<p><b>BreakDown:</b></p>\r\n",
    "<ol type=\"A\">\r\n",
    "    <li><b>Model Development</b>\r\n",
    "        <ol type=\"1\">\r\n",
    "            <li>Data Feature Engineering</li>\r\n",
    "            <li>Model Architecture</li>\r\n",
    "            <li>Training</li>\r\n",
    "            <li>Validation</li>\r\n",
    "            <li>Testing</li>\r\n",
    "            <li>Backtesting</li>\r\n",
    "        </ol>\r\n",
    "    </li>\r\n",
    "    <li><b>Exporting and Deploying Model</b>\r\n",
    "        <ol type=\"1\" start=\"7\">\r\n",
    "            <li>Export the Model from PyTorch to ONNX</li>\r\n",
    "            <li>Deploy the Model in a Backend</li>\r\n",
    "        </ol>\r\n",
    "    </li>\r\n",
    "</ol>\r\n",
    "</div>\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c5b2ab-4d45-4f13-a07b-59477686c67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n",
    "from tqdm import tqdm  # Progress bar for batch processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85426d0b-f6cc-4a90-9fbb-bc0d14ed1a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2025 NVIDIA Corporation\n",
      "Built on Wed_Jan_15_19:38:46_Pacific_Standard_Time_2025\n",
      "Cuda compilation tools, release 12.8, V12.8.61\n",
      "Build cuda_12.8.r12.8/compiler.35404655_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd5eba68-543b-4c62-8e52-fd9596b18a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # Should return a CUDA version\n",
    "print(torch.backends.cudnn.enabled)  # Should return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab5a1d75-dc81-4d98-a603-448d8b332085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)  # Should return 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "645815a7-0e73-4cf8-b1b0-bdb169b37b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available())  \u001b[38;5;66;03m# Should return True\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count())  \u001b[38;5;66;03m# Should return a number greater than 0\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_device_name(\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:414\u001b[0m, in \u001b[0;36mget_device_name\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_name\u001b[39m(device: Optional[_device_t] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    403\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \n\u001b[0;32m    405\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;124;03m        str: the name of the device\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_device_properties(device)\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:444\u001b[0m, in \u001b[0;36mget_device_properties\u001b[1;34m(device)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device_properties\u001b[39m(device: _device_t) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _CudaDeviceProperties:\n\u001b[0;32m    435\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[0;32m    436\u001b[0m \n\u001b[0;32m    437\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    442\u001b[0m \u001b[38;5;124;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[0;32m    443\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 444\u001b[0m     _lazy_init()  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[0;32m    445\u001b[0m     device \u001b[38;5;241m=\u001b[39m _get_device_index(device, optional\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count():\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 293\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())  # Should return True\n",
    "print(torch.cuda.device_count())  # Should return a number greater than 0\n",
    "print(torch.cuda.get_device_name(0))  # Should return the GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "740c3fc0-4116-419d-8e34-a9bcc36a5645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 50000\n",
      "Total tokenized sequences: 4856\n",
      "Data Split: 3399 training, 971 validation, 486 testing sequences.\n",
      "DataLoaders ready:\n",
      " - Train batches: 54\n",
      " - Validation batches: 16\n",
      " - Test batches: 8\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 124\u001b[0m\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Example: Using DataLoader in Training Loop\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m--> 124\u001b[0m     batch \u001b[38;5;241m=\u001b[39m batch\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Move batch to GPU\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Example output: [batch_size, max_seq_length]\u001b[39;00m\n\u001b[0;32m    126\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\cuda\\__init__.py:293\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[0;32m    292\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 293\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_cuda_init()\n\u001b[0;32m    294\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[0;32m    295\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[0;32m    296\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[0;32m    297\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Step 1: Load Vocabulary\n",
    "# ================================================================\n",
    "\n",
    "# Load the tokenized vocabulary (vocab.json)\n",
    "with open(\"vocab.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    vocab = json.load(f)\n",
    "\n",
    "# Create token-to-ID and ID-to-token mappings\n",
    "token_to_id = vocab\n",
    "id_to_token = {v: k for k, v in vocab.items()}\n",
    "\n",
    "print(f\"Vocabulary size: {len(vocab)}\")\n",
    "\n",
    "# ================================================================\n",
    "# Step 2: Define Tokenized Dataset Class\n",
    "# ================================================================\n",
    "\n",
    "class TokenizedDataset(Dataset):\n",
    "    \"\"\"\n",
    "    PyTorch Dataset for tokenized sequences.\n",
    "    - Handles padding and truncation to ensure uniform sequence length.\n",
    "    \"\"\"\n",
    "    def __init__(self, data, max_seq_length=4096):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - data: List of tokenized sequences (each a list of token IDs).\n",
    "        - max_seq_length: Fixed length for padding/truncation.\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.max_seq_length = max_seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns a padded/truncated tokenized sequence as a PyTorch tensor.\n",
    "        \"\"\"\n",
    "        tokens = self.data[idx]\n",
    "        \n",
    "        # Pad or truncate the sequence to max_seq_length\n",
    "        if len(tokens) < self.max_seq_length:\n",
    "            tokens += [token_to_id[\"<pad>\"]] * (self.max_seq_length - len(tokens))\n",
    "        else:\n",
    "            tokens = tokens[:self.max_seq_length]\n",
    "        \n",
    "        return torch.tensor(tokens, dtype=torch.long)\n",
    "\n",
    "# ================================================================\n",
    "# Step 3: Load WET Tokenized Data\n",
    "# ================================================================\n",
    "\n",
    "# Load the wet_tokenized.json file\n",
    "#We will have 20 different WET files to show the different sequences\n",
    "#For a larger language model we are going to need alot more cleaned WET Files\n",
    "with open(\"wet_tokenized.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    wet_tokenized_data = json.load(f)\n",
    "\n",
    "# Extract token IDs from the dataset\n",
    "tokenized_sequences = [entry[\"token_ids\"] for entry in wet_tokenized_data]\n",
    "\n",
    "print(f\"Total tokenized sequences: {len(tokenized_sequences)}\")\n",
    "\n",
    "# ================================================================\n",
    "# Step 4: Split Data into Train/Validation/Test\n",
    "# ================================================================\n",
    "\n",
    "# Define split sizes (70% train, 20% validation, 10% test)\n",
    "total_sequences = len(tokenized_sequences)\n",
    "train_size = int(0.7 * total_sequences)\n",
    "val_size = int(0.2 * total_sequences)\n",
    "test_size = total_sequences - train_size - val_size\n",
    "\n",
    "# Perform the split\n",
    "train_data, val_data, test_data = random_split(tokenized_sequences, [train_size, val_size, test_size])\n",
    "\n",
    "print(f\"Data Split: {len(train_data)} training, {len(val_data)} validation, {len(test_data)} testing sequences.\")\n",
    "\n",
    "# ================================================================\n",
    "# Step 5: Create DataLoaders for Batch Processing\n",
    "# ================================================================\n",
    "\n",
    "# Set batch size (optimized for GPU memory)\n",
    "batch_size = 64\n",
    "max_seq_length = 4096\n",
    "\n",
    "# Create PyTorch Datasets\n",
    "train_dataset = TokenizedDataset(train_data, max_seq_length=max_seq_length)\n",
    "val_dataset = TokenizedDataset(val_data, max_seq_length=max_seq_length)\n",
    "test_dataset = TokenizedDataset(test_data, max_seq_length=max_seq_length)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "\n",
    "print(f\"DataLoaders ready:\")\n",
    "print(f\" - Train batches: {len(train_loader)}\")\n",
    "print(f\" - Validation batches: {len(val_loader)}\")\n",
    "print(f\" - Test batches: {len(test_loader)}\")\n",
    "\n",
    "# ================================================================\n",
    "# Notes for the Model Architecture\n",
    "# ================================================================\n",
    "\n",
    "\"\"\"\n",
    "HOW THIS IS PROCESSED:\n",
    "- Each DataLoader yields batches of size `batch_size`.\n",
    "- Each batch contains tokenized sequences padded/truncated to `max_seq_length`.\n",
    "- These batches are directly fed into the Transformer model during training.\n",
    "\n",
    "For example:\n",
    "- Train DataLoader yields a batch of shape [batch_size, max_seq_length].\n",
    "- The batch is moved to the GPU (Titan RTX) and passed to the model.\n",
    "\n",
    "Optimized GPU Utilization:\n",
    "- Larger `batch_size` takes advantage of the GPU's memory.\n",
    "- Use `pin_memory=True` for faster data transfer between CPU and GPU.\n",
    "\"\"\"\n",
    "\n",
    "# Example: Using DataLoader in Training Loop\n",
    "for batch in train_loader:\n",
    "    batch = batch.to(\"cuda\")  # Move batch to GPU\n",
    "    print(f\"Batch shape: {batch.shape}\")  # Example output: [batch_size, max_seq_length]\n",
    "    break  # Stop after the first batch to illustrate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d7b76d-6e2c-44dd-a4d4-f05cd76b3ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Architecture\n",
    "\"\"\"\n",
    "The model architecture is a transfoer model which is inspired and made from the following paper, Attention is All You Need\n",
    "\n",
    "and the similar technique, Textbooks is All You Need, apply the transformer model, but using textbook like data from a mix of resources, including\n",
    "a majority of it from common crawl \n",
    "\n",
    "There are 4 parts to a transformer model\n",
    "1. Self Attention Layer--->Pays attention to the inputs and sees the probability of what to \"predict\" based on a score\n",
    "2. Feed Forward Network-->Processes hwo the data will be made with the data\n",
    "3. Transformer Block = Combines the Self Attention Layer with the Feed Forward Network\n",
    "4. Transformer Model with its necessary parameters and adding in the number of heads, embeddings, hidden layers, and \n",
    "\"\"\"\n",
    "# ================================================================\n",
    "#  PART 1: MULTI-HEAD SELF-ATTENTION LAYER (Uses Softmax for Attention Weights)\n",
    "# ================================================================\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention Layer:\n",
    "    - Computes self-attention for multiple \"heads\" in parallel.\n",
    "    - Uses Softmax for probability-based attention weights.\n",
    "\n",
    "    Parameters:\n",
    "    - embed_dim: The dimension of the input embeddings.\n",
    "    - num_heads: Number of attention heads.\n",
    "\n",
    "    Operations:\n",
    "    - Projects input embeddings into queries, keys, and values.\n",
    "    - Computes scaled dot-product attention.\n",
    "    - Applies Softmax to normalize attention scores.\n",
    "    - Merges attention outputs and projects them back to embedding space.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by num_heads.\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Learnable projection matrices for Q, K, V\n",
    "        self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass of Multi-Head Self-Attention.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        - mask: Optional mask to prevent attending to certain positions.\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, embed_dim = x.shape\n",
    "\n",
    "        # Project input to Queries, Keys, and Values\n",
    "        qkv = self.qkv_proj(x).reshape(batch_size, seq_length, self.num_heads, 3 * self.head_dim).permute(2, 0, 1, 3)\n",
    "        queries, keys, values = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Compute attention scores (scaled dot-product)\n",
    "        attention_scores = torch.matmul(queries, keys.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "\n",
    "        # Apply optional mask (for masked attention)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Apply Softmax to normalize attention scores\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "\n",
    "        # Compute attention output\n",
    "        context = torch.matmul(attention_weights, values).permute(1, 2, 0, 3).reshape(batch_size, seq_length, embed_dim)\n",
    "        \n",
    "        return self.out_proj(context)\n",
    "\n",
    "# ================================================================\n",
    "#  PART 2: FEEDFORWARD NETWORK (FFN) WITH GELU\n",
    "# ================================================================\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    FeedForward Network (FFN):\n",
    "    - Applies a non-linear transformation to each token representation.\n",
    "    - Uses GELU \n",
    "\n",
    "    Parameters:\n",
    "    - embed_dim: Input and output dimension.\n",
    "    - hidden_dim: Expanded dimension inside the feedforward network.\n",
    "\n",
    "    Operations:\n",
    "    - Applies Linear â†’ GELU â†’ Linear transformations.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
    "        self.gelu = nn.GELU()  # Using GELU\n",
    "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of FFN.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Tensor of shape (batch_size, seq_length, embed_dim)\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "        return self.fc2(self.gelu(self.fc1(x)))\n",
    "\n",
    "# ================================================================\n",
    "#  PART 3: TRANSFORMER ENCODER BLOCK (Combining Attention & FFN)\n",
    "# ================================================================\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer Encoder Block:\n",
    "    - Combines Multi-Head Self-Attention and FeedForward Network (FFN).\n",
    "    - Uses Layer Normalization for stability.\n",
    "    - Applies residual connections to improve gradient flow.\n",
    "\n",
    "    Parameters:\n",
    "    - embed_dim: The embedding dimension.\n",
    "    - num_heads: The number of attention heads.\n",
    "    - hidden_dim: The size of the hidden layer in FFN.\n",
    "\n",
    "    This block is stacked multiple times in the full Transformer.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Multi-Head Self-Attention Layer\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "\n",
    "        # Layer Normalization for stable learning\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Feedforward Network\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, hidden_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through Transformer Encoder Block.\n",
    "\n",
    "        Inputs:\n",
    "        - x: Input tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        - mask: Optional attention mask\n",
    "\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, seq_length, embed_dim)\n",
    "        \"\"\"\n",
    "        # Apply Self-Attention with Residual Connection\n",
    "        attn_output = self.attention(x, mask)\n",
    "        x = self.norm1(x + attn_output)  \n",
    "\n",
    "        # Apply FeedForward Network with Residual Connection\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)  \n",
    "\n",
    "        return x\n",
    "\n",
    "# ================================================================\n",
    "#  PART 4: FULL TRANSFORMER MODEL (25 Billion Parameters)\n",
    "# ================================================================\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full Transformer Model:\n",
    "    - Consists of Token Embeddings, Positional Encoding, Transformer Blocks, and Output Layer.\n",
    "    - Uses GELU for all activation functions.\n",
    "\n",
    "    Parameters:\n",
    "    - vocab_size: Number of unique tokens in vocabulary.\n",
    "    - embed_dim: Size of the word embeddings.\n",
    "    - num_heads: Number of attention heads per layer.\n",
    "    - num_layers: Number of stacked Transformer layers.\n",
    "    - hidden_dim: Size of the hidden layer in FFN.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim=12288, num_heads=96, num_layers=96, hidden_dim=49152):\n",
    "        super().__init__()\n",
    "\n",
    "        # Token Embedding Layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # Positional Encoding (learnable)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, 4096, embed_dim))\n",
    "\n",
    "        # Transformer Layers (Stacked Encoder Blocks)\n",
    "        self.encoder_layers = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, hidden_dim)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "        # Final projection layer to vocabulary size\n",
    "        self.fc_out = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the full Transformer.\n",
    "\n",
    "        Inputs:\n",
    "        - input_ids: Tensor of tokenized input (batch_size, seq_length)\n",
    "        - attention_mask: Optional mask to prevent attending to padding tokens.\n",
    "\n",
    "        Returns:\n",
    "        - Logits of shape (batch_size, seq_length, vocab_size)\n",
    "        \"\"\"\n",
    "        # Token Embeddings\n",
    "        x = self.embedding(input_ids)\n",
    "\n",
    "        # Add Positional Encoding\n",
    "        x = x + self.positional_encoding[:, :x.size(1), :]\n",
    "\n",
    "        # Pass through all Transformer Encoder Blocks\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, attention_mask)\n",
    "\n",
    "        # Output projection\n",
    "        logits = self.fc_out(x)\n",
    "        return logits\n",
    "\n",
    "# ================================================================\n",
    "#  PART 5: MODEL CONFIGURATION & TESTING\n",
    "# ================================================================\n",
    "if __name__ == \"__main__\":\n",
    "    vocab_size = 50000\n",
    "    max_seq_length = 4096\n",
    "    model = TransformerModel(vocab_size)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\" Transformer Model with {total_params:,} Parameters\")\n",
    "\n",
    "    input_ids = torch.randint(0, vocab_size, (2, max_seq_length))\n",
    "    output = model(input_ids)\n",
    "    print(\"Output shape:\", output.shape)  # Expected: (batch_size, seq_length, vocab_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50102533-042b-4d7f-9e89-403ec2b50891",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train\n",
    "\"\"\"\"\n",
    "When we train a given neural network, what we are doing is first splitting our data into 70:20:10, or can do 80:10:10, where in this case, the 70\n",
    "is going to be the majority of our data which we will train with our given model, and during training we are going to be having the following criteria\n",
    "\n",
    "1. Batch processing\n",
    "\n",
    "2. Number of Epochs\n",
    "\n",
    "3. Corresponding loss score\n",
    "\n",
    "4. Early Stopping\n",
    "\n",
    "*We will use matplot lib to show the plot of epochs to the loss score, along with the F1 score, confusion matrix, then we will apply validation,\n",
    "testing, and backtesting\n",
    "\n",
    "\"\"\"\"\n",
    "# ================================================================\n",
    "# ðŸš€ TRAINING FUNCTION WITH EARLY STOPPING, LOSS VISUALIZATION, & PROGRESS BAR\n",
    "# ================================================================\n",
    "\n",
    "def train_model(model, train_loader, val_loader, epochs=10, lr=2e-5, patience=3, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Trains the Transformer model and tracks performance metrics with a progress bar.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Transformer model (25B parameters).\n",
    "    - train_loader: DataLoader for training data.\n",
    "    - val_loader: DataLoader for validation data.\n",
    "    - epochs: Maximum number of epochs to train.\n",
    "    - lr: Learning rate for AdamW optimizer.\n",
    "    - patience: Number of epochs to wait before early stopping.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "    \n",
    "    Returns:\n",
    "    - Trained model with the best validation performance.\n",
    "    \"\"\"\n",
    "    \n",
    "    model.to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        # Add progress bar for training batches\n",
    "        train_progress = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs} - Training\", leave=False)\n",
    "\n",
    "        for batch in train_progress:\n",
    "            batch = batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), batch.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            # Update progress bar with current loss\n",
    "            train_progress.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "        avg_val_loss = validate_model(model, val_loader, loss_fn, device)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {avg_train_loss:.4f} | Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_transformer_model.pth\")\n",
    "            print(\"Model saved (Best Validation Loss Improved).\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered. Training stopped.\")\n",
    "                break\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss', linestyle='dashed')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training vs. Validation Loss Over Epochs')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2279959e-e184-4d8d-8d46-8951f2adef36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Validate\n",
    "\"\"\"\n",
    "Validation Function\n",
    "\"\"\"\n",
    "# ================================================================\n",
    "# ðŸš€ VALIDATION FUNCTION WITH PROGRESS BAR\n",
    "# ================================================================\n",
    "\n",
    "def validate_model(model, val_loader, loss_fn, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates the model on validation data and computes key performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Transformer model.\n",
    "    - val_loader: DataLoader for validation data.\n",
    "    - loss_fn: Loss function.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "    \n",
    "    Returns:\n",
    "    - Average validation loss.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    val_progress = tqdm(val_loader, desc=\"Validation\", leave=False)  # Progress bar for validation\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_progress:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), batch.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=-1).view(-1).cpu().numpy()\n",
    "            labels = batch.view(-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            # Update progress bar with current batch loss\n",
    "            val_progress.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_val_loss = total_loss / len(val_loader)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix - Validation\")\n",
    "    plt.show()\n",
    "\n",
    "    return avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf08cd9-da48-4fd6-916c-04dcc47bad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "\"\"\"\n",
    "Test Function\n",
    "\"\"\"\n",
    "# ================================================================\n",
    "# ðŸš€ TEST FUNCTION WITH PROGRESS BAR\n",
    "# ================================================================\n",
    "\n",
    "def test_model(model, test_loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Evaluates the trained model on test data and computes final metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: Transformer model.\n",
    "    - test_loader: DataLoader for test data.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "    \n",
    "    Returns:\n",
    "    - Perplexity score (lower is better).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = []\n",
    "\n",
    "    test_progress = tqdm(test_loader, desc=\"Testing\", leave=False)  # Progress bar for testing\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_progress:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), batch.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = torch.argmax(outputs, dim=-1).view(-1).cpu().numpy()\n",
    "            labels = batch.view(-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "            # Update progress bar with current batch loss\n",
    "            test_progress.set_postfix({\"Batch Loss\": loss.item()})\n",
    "\n",
    "    avg_test_loss = total_loss / len(test_loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_test_loss))\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"weighted\")\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Test Perplexity: {perplexity:.2f}, Accuracy: {accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.xlabel(\"Predicted Labels\")\n",
    "    plt.ylabel(\"True Labels\")\n",
    "    plt.title(\"Confusion Matrix - Test\")\n",
    "    plt.show()\n",
    "\n",
    "    return perplexity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837c45bf-b972-43cc-9fbe-09731c02151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Backtest\n",
    "\"\"\"\n",
    "Backtesting is the final benchmarking phase depneding how one wants to apply their model, and we will be using 4 types of backtests for our language model\n",
    "1. Perplexity score: Measure Predictability\n",
    "2. BLEU Score: Measures Similarity to Reference Texts\n",
    "3. ROGUE Score: Measures Uniqueness of Generated Text\n",
    "4. Test Divedrsity Score: Measures Uniqueness of Generated Text\n",
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge_score import rouge_scorer\n",
    "from collections import Counter\n",
    "\n",
    "# ================================================================\n",
    "# ðŸš€ PERPLEXITY SCORE (Measures Predictability)\n",
    "# ================================================================\n",
    "\n",
    "def calculate_perplexity(model, test_loader, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Computes perplexity (PPL) on test data.\n",
    "    - Lower perplexity means the model predicts tokens more accurately.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The trained Transformer model.\n",
    "    - test_loader: DataLoader with test data.\n",
    "    - device: \"cuda\" or \"cpu\".\n",
    "\n",
    "    Returns:\n",
    "    - Perplexity score (lower = better).\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding token (0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            outputs = model(batch)  # Model predicts token probabilities\n",
    "            loss = loss_fn(outputs.view(-1, outputs.size(-1)), batch.view(-1))  # Compute loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(test_loader)\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))  # e^(cross-entropy loss)\n",
    "\n",
    "    print(f\"Perplexity Score: {perplexity:.2f} (Lower is better)\")\n",
    "    return perplexity\n",
    "\n",
    "# ================================================================\n",
    "# ðŸš€ BLEU SCORE (Measures Similarity to Reference Texts)\n",
    "# ================================================================\n",
    "\n",
    "def calculate_bleu(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Computes BLEU score between reference texts and model-generated texts.\n",
    "    - BLEU measures how close generated text is to human-written text.\n",
    "\n",
    "    Parameters:\n",
    "    - reference_texts: List of ground truth text samples.\n",
    "    - generated_texts: List of model-generated samples.\n",
    "\n",
    "    Returns:\n",
    "    - Average BLEU score (higher = better).\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for ref, gen in zip(reference_texts, generated_texts):\n",
    "        reference = [ref.split()]  # Reference text split into words\n",
    "        candidate = gen.split()  # Model-generated text split into words\n",
    "        score = sentence_bleu(reference, candidate)  # Compute BLEU score\n",
    "        scores.append(score)\n",
    "\n",
    "    avg_bleu = sum(scores) / len(scores)  # Compute average BLEU across samples\n",
    "    print(f\"BLEU Score: {avg_bleu:.4f} (Higher is better)\")\n",
    "    return avg_bleu\n",
    "\n",
    "# ================================================================\n",
    "# ðŸš€ ROUGE SCORE (Measures Recall & Overlap with Reference Texts)\n",
    "# ================================================================\n",
    "\n",
    "def calculate_rouge(reference_texts, generated_texts):\n",
    "    \"\"\"\n",
    "    Computes ROUGE scores for model evaluation.\n",
    "    - ROUGE measures how much of the reference text appears in the generated text.\n",
    "\n",
    "    Parameters:\n",
    "    - reference_texts: List of human-written reference texts.\n",
    "    - generated_texts: List of model-generated outputs.\n",
    "\n",
    "    Returns:\n",
    "    - ROUGE scores (Higher is better).\n",
    "    \"\"\"\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = [scorer.score(ref, gen) for ref, gen in zip(reference_texts, generated_texts)]\n",
    "\n",
    "    # Compute average ROUGE scores across all test samples\n",
    "    avg_rouge = {\n",
    "        'rouge1': sum(s['rouge1'].fmeasure for s in scores) / len(scores),\n",
    "        'rouge2': sum(s['rouge2'].fmeasure for s in scores) / len(scores),\n",
    "        'rougeL': sum(s['rougeL'].fmeasure for s in scores) / len(scores)\n",
    "    }\n",
    "\n",
    "    print(f\"ROUGE Scores: {avg_rouge}\")\n",
    "    return avg_rouge\n",
    "\n",
    "# ================================================================\n",
    "# ðŸš€ TEXT DIVERSITY SCORE (Measures Uniqueness of Generated Text)\n",
    "# ================================================================\n",
    "\n",
    "def measure_text_diversity(generated_texts, n=2):\n",
    "    \"\"\"\n",
    "    Measures diversity in model-generated text by analyzing distinct n-grams.\n",
    "    - Ensures that generated text is not repetitive or generic.\n",
    "\n",
    "    Parameters:\n",
    "    - generated_texts: List of generated text outputs.\n",
    "    - n: N-gram size (default = 2 for bigrams).\n",
    "\n",
    "    Returns:\n",
    "    - Distinct n-gram ratio (higher = more diverse).\n",
    "    \"\"\"\n",
    "    all_ngrams = []\n",
    "    total_ngrams = 0\n",
    "\n",
    "    for text in generated_texts:\n",
    "        words = text.split()\n",
    "        ngrams = [tuple(words[i:i+n]) for i in range(len(words) - n + 1)]\n",
    "        all_ngrams.extend(ngrams)\n",
    "        total_ngrams += len(ngrams)\n",
    "\n",
    "    unique_ngrams = len(set(all_ngrams))\n",
    "    diversity_score = unique_ngrams / total_ngrams if total_ngrams > 0 else 0\n",
    "\n",
    "    print(f\"Diversity Score (Distinct-{n} Ratio): {diversity_score:.4f} (Higher is better)\")\n",
    "    return diversity_score\n",
    "\n",
    "# ================================================================\n",
    "# ðŸš€ RUN THE FULL BACKTESTING PIPELINE\n",
    "# ================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    Runs all backtesting benchmarks for a trained language model.\n",
    "    - Loads reference and generated text.\n",
    "    - Computes Perplexity, BLEU, ROUGE, and Diversity Scores.\n",
    "    \"\"\"\n",
    "\n",
    "    # Example reference and generated texts (For real evaluation, use a larger dataset)\n",
    "    reference_texts = [\"The cat sat on the mat.\", \"Artificial intelligence is advancing rapidly.\"]\n",
    "    generated_texts = [\"The cat lay on the rug.\", \"AI is progressing very fast.\"]\n",
    "\n",
    "    # Load trained model and test data\n",
    "    model = TransformerModel(vocab_size=50000)  # Load your trained model\n",
    "    model.load_state_dict(torch.load(\"best_transformer_model.pth\"))  # Load best model\n",
    "    model.eval()\n",
    "\n",
    "    # Define test DataLoader\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    print(\"\\n===== Backtesting Results =====\\n\")\n",
    "\n",
    "    # Compute Perplexity (Lower is Better)\n",
    "    perplexity = calculate_perplexity(model, test_loader, device=\"cuda\")\n",
    "\n",
    "    # Compute BLEU Score (Higher is Better)\n",
    "    bleu_score = calculate_bleu(reference_texts, generated_texts)\n",
    "\n",
    "    # Compute ROUGE Score (Higher is Better)\n",
    "    rouge_scores = calculate_rouge(reference_texts, generated_texts)\n",
    "\n",
    "    # Compute Diversity Score (Higher is Better)\n",
    "    diversity_score = measure_text_diversity(generated_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85de8aa6-cf8e-4777-85dd-c0350d1a0487",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export\n",
    "\"\"\"\n",
    "We will convert our Pytorch model to ONNX format to then save the model locally prior to deploying it on Microsoft Azure backend's servers so everyone\n",
    "can use the following model and interface with it \n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
